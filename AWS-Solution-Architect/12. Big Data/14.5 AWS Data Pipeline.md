[[Big Data]], [[pipeline]]

## AWS Data Pipeline

AWS Data Pipeline is a managed **Extract, Transform, Load (ETL)** service for automating movement and transformation of the data

### Overview

- **Data Driven**: Define data-driven workflows. Steps are dependent on previous task completing successfully
- **Parameters**: Define you parameters for data transformation. AWS Data Pipeline enforces your chosen login.
- **HA**: AWS hosts the infra
- **Handling Failures**: Automatically retries failed activities and can set up SNS
- **Storage**: Integrate with DynamoDB, RDS, Redshift and S3
- **Compute**: Work on EC2

### Components to know

- **Pipeline Definition**: Specify the business logic of the data management needs
- **Managed Compute**: Service will create EC2 to perform activities or leverage existing EC2
- **Task runner** (EC2): Poll for different tasks and perform when found
- **Data Nodes**: Define the locations and types of data that will be input and output

***Activities are pipeline components that define the work to perform***

### Use cases

- Processing data in EMR using Hadoop streaming
- Importing/Exporting DynamoDB data
- Copying CSV files or data between S3
- Exporting RDS data to S3
- Copying data to Redshift

# Exam Tips

Know everything above
